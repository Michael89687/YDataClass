{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datascience import *\n",
    "import numpy as np\n",
    "import re\n",
    "import gensim\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "logging.root.level = logging.CRITICAL \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "# direct plots to appear within the cell, and set their style\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plots\n",
    "plots.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"https://s3.amazonaws.com/sds171/labs/lab07/ted_talks.csv\"\n",
    "data = Table.read_table(filename)\n",
    "\n",
    "transcripts = data.column('transcript')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using regular expression to clean the data\n",
    "transcripts = [re.sub('-', ' ', plot) for plot in transcripts]\n",
    "transcripts = [re.sub('[^\\w\\s]', '', plot) for plot in transcripts]\n",
    "transcripts = [re.sub('[A-Z]\\w*', '', plot) for plot in transcripts]\n",
    "transcripts = [re.sub('[ ]+', ' ', plot) for plot in transcripts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_numeric(string):\n",
    "    for char in string:\n",
    "        if char.isdigit():\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def has_poss_contr(string):\n",
    "    for i in range(len(string) - 1):\n",
    "        if string[i] == '\\'' and string[i+1] == 's':\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def empty_string(string):\n",
    "    return string == ''\n",
    "\n",
    "def remove_string(string):\n",
    "    return is_numeric(string) | has_poss_contr(string) | empty_string(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize\n",
    "plots_tok = []\n",
    "for plot in transcripts:\n",
    "    processed = plot.lower().strip().split(' ')\n",
    "    plots_tok.append(processed)\n",
    "\n",
    "#Removing numeric, posessives/contractions, and empty strings\n",
    "temp = []\n",
    "for plot in plots_tok:\n",
    "    filtered = []\n",
    "    for token in plot:\n",
    "        if not remove_string(token):\n",
    "            filtered.append(token)\n",
    "    temp.append(filtered)\n",
    "plots_tok = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/michaelchau/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "#Lemmatizing the tokens \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "temp = []\n",
    "for plot in plots_tok:\n",
    "    processed = []\n",
    "    for token in plot:\n",
    "        processed.append(lemmatizer.lemmatize(token, pos='v'))\n",
    "    temp.append(processed)\n",
    "plots_tok = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 33616\n"
     ]
    }
   ],
   "source": [
    "#Creating the Counter\n",
    "vocab = Counter()\n",
    "for plot in plots_tok:\n",
    "    vocab.update(plot)\n",
    "\n",
    "print(\"Number of unique tokens: %d\" % len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 7101\n"
     ]
    }
   ],
   "source": [
    "#Keeping tokens that appear more than 20 times \n",
    "tokens = []\n",
    "for token in vocab.elements():\n",
    "    if vocab[token] > 20:\n",
    "        tokens.append(token)\n",
    "vocab = Counter(tokens)\n",
    "\n",
    "print(\"Number of unique tokens: %d\" % len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 6901\n"
     ]
    }
   ],
   "source": [
    "#Removing rare and stop words\n",
    "stop_words = []\n",
    "for item in vocab.most_common(200):\n",
    "    stop_word = item[0]\n",
    "    stop_words.append(stop_word)\n",
    "tokens = []\n",
    "for token in vocab.elements():\n",
    "    if token not in stop_words:\n",
    "        tokens.append(token)\n",
    "vocab = Counter(tokens)\n",
    "\n",
    "print(\"Number of unique tokens: %d\" % len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens mapped: 6901\n",
      "Identifier for 'photograph': 2252\n",
      "Word for identifier 2252: photograph\n"
     ]
    }
   ],
   "source": [
    "#Creating the identifier mappings word2id and id2word\n",
    "items = vocab.items()\n",
    "id2word = {}\n",
    "word2id = {}\n",
    "idx = 0\n",
    "for word, count in vocab.items():\n",
    "    id2word[idx] = word\n",
    "    word2id[word] = idx\n",
    "    idx += 1\n",
    "    \n",
    "print(\"Number of tokens mapped: %d\" % len(id2word))\n",
    "print(\"Identifier for 'photograph': %d\" % word2id['photograph'])\n",
    "print(\"Word for identifier %d: %s\" % (word2id['photograph'], id2word[word2id['photograph']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering the tokens \n",
    "temp = []\n",
    "for plot in plots_tok:\n",
    "    filtered = []\n",
    "    for token in plot:\n",
    "        if token in vocab:\n",
    "            filtered.append(token)\n",
    "    temp.append(filtered)\n",
    "plots_tok = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot, tokenized:\n",
      " ['stuff', 'book', 'mine', 'hope', 'resonate', 'youve', 'already', 'connections', 'myself', 'case', 'miss', 'official', 'official', 'official', 'industrial', 'societies', 'official', 'run', 'maximize', 'welfare', 'citizens', 'maximize', 'individual', 'freedom', 'reason', 'both', 'freedom', 'itself', 'valuable', 'worthwhile', 'essential', 'freedom', 'act', 'maximize', 'welfare', 'decide', 'behalf', 'maximize', 'freedom', 'maximize', 'choice', 'choice', 'freedom', 'freedom', 'welfare', 'deeply', 'embed', 'water', 'supply', 'wouldnt', 'occur', 'anyone', 'deeply', 'embed', 'examples', 'modern', 'progress', 'possible', 'supermarket', 'such', 'word', 'salad', 'dress', 'salad', 'dress', 'supermarket', 'count', 'extra', 'virgin', 'olive', 'oil', 'buy', 'large', 'number', 'salad', 'dress', 'chance', 'none', 'store', 'offer', 'suit', 'supermarket', 'consumer', 'electronics', 'store', 'set', 'stereo', 'speakers', 'player', 'tape', 'player', 'single', 'consumer', 'electronics', 'store', 'stereo', 'systems', 'construct', 'six', 'half', 'million', 'stereo', 'systems', 'components', 'offer', 'store', 'admit', 'choice', 'domains', 'communications', 'boy', 'telephone', 'service', 'rent', 'phone', 'buy', 'consequence', 'phone', 'break', 'days', 'almost', 'unlimited', 'variety', 'phone', 'especially', 'cell', 'phone', 'cell', 'phone', 'future', 'favorite', 'middle', 'player', 'nose', 'hair', 'chance', 'havent', 'store', 'yet', 'rest', 'assure', 'soon', 'lead', 'walk', 'store', 'answer', 'answer', 'possible', 'buy', 'cell', 'phone', 'doesnt', 'too', 'aspects', 'significant', 'buy', 'explosion', 'choice', 'true', 'care', 'longer', 'case', 'doctor', 'doctor', 'doctor', 'doctor', 'benefit', 'risk', 'benefit', 'risk', 'benefit', 'risk', 'benefit', 'risk', 'result', 'patient', 'autonomy', 'sound', 'shift', 'burden', 'responsibility', 'decision', 'somebody', 'namely', 'doctor', 'somebody', 'nothing', 'almost', 'certainly', 'sick', 'thus', 'best', 'shape', 'decisions', 'namely', 'patient', 'enormous', 'market', 'prescription', 'drug', 'sense', 'since', 'buy', 'market', 'buy', 'answer', 'expect', 'doctor', 'morning', 'dramatic', 'identity', 'matter', 'choice', 'slide', 'indicate', 'inherit', 'identity', 'invent', 're', 'invent', 'ourselves', 'often', 'wake', 'morning', 'decide', 'person', 'respect', 'marriage', 'family', 'default', 'assumption', 'almost', 'everyone', 'marry', 'soon', 'soon', 'real', 'choice', 'everything', 'grab', 'teach', 'wonderfully', 'intelligent', 'students', 'assign', 'less', 'less', 'smart', 'less', 'themselves', 'marry', 'marry', 'marry', 'later', 'career', 'consume', 'answer', 'whether', 'assign', 'grade', 'indeed', 'answer', 'bless', 'technology', 'enable', 'minute', 'planet', 'except', 'corner', 'anybody', 'incredible', 'freedom', 'choice', 'respect', 'decision', 'again', 'again', 'again', 'whether', 'shouldnt', 'watch', 'play', 'soccer', 'cell', 'phone', 'hip', 'hip', 'laptop', 'presumably', 'lap', 'shut', 'minute', 'watch', 'mutilate', 'soccer', 'game', 'ourselves', 'answer', 'cell', 'phone', 'respond', 'email', 'draft', 'letter', 'answer', 'certainly', 'experience', 'soccer', 'game', 'wouldve', 'everywhere', 'small', 'material', 'lifestyle', 'matter', 'choice', 'write', 'stone', 'choices', 'everything', 'matter', 'choice', 'news', 'bad', 'news', 'answer', 'yes', 'whats', 'whats', 'bad', 'choice', 'effect', 'negative', 'effect', 'effect', 'produce', 'paralysis', 'rather', 'liberation', 'options', 'choose', 'difficult', 'choose', 'dramatic', 'example', 'study', 'investments', 'voluntary', 'retirement', 'plan', 'colleague', 'mine', 'access', 'investment', 'record', 'gigantic', 'mutual', 'fund', 'company', 'million', 'employees', 'mutual', 'fund', 'employer', 'offer', 'rate', 'participation', 'offer', 'fund', 'fewer', 'employees', 'participate', 'offer', 'five', 'fund', 'choose', 'damn', 'hard', 'decide', 'fund', 'choose', 'youll', 'until', 'tomorrow', 'tomorrow', 'tomorrow', 'tomorrow', 'tomorrow', 'eat', 'dog', 'food', 'retire', 'enough', 'money', 'away', 'decision', 'hard', 'pass', 'significant', 'match', 'money', 'employer', 'participate', 'pass', 'dollars', 'employer', 'happily', 'match', 'contribution', 'paralysis', 'consequence', 'too', 'choices', 'lastly', 'eternity', 'cheese', 'ranch', 'decision', 'eternity', 'pick', 'wrong', 'mutual', 'fund', 'wrong', 'salad', 'dress', 'effect', 'second', 'effect', 'manage', 'overcome', 'paralysis', 'choice', 'less', 'satisfy', 'result', 'choice', 'fewer', 'options', 'choose', 'several', 'reason', 'salad', 'dress', 'choose', 'buy', 'perfect', 'salad', 'dress', 'easy', 'imagine', 'choice', 'imagine', 'alternative', 'induce', 'regret', 'decision', 'regret', 'satisfaction', 'decision', 'decision', 'options', 'easier', 'regret', 'anything', 'disappoint', 'option', 'choose', 'economists', 'opportunity', 'cost', 'morning', 'value', 'depend', 'compare', 'alternatives', 'consider', 'easy', 'imagine', 'attractive', 'feature', 'alternatives', 'reject', 'less', 'satisfy', 'alternative', 'youve', 'choose', 'example', 'stop', 'available', 'park', 'space', 'street', 'suppose', 'couple', 'expensive', 'real', 'estate', 'beach', 'themselves', 'damn', 'guy', 'neighborhood', 'away', 'park', 'front', 'spend', 'weeks', 'miss', 'opportunity', 'park', 'space', 'cost', 'satisfaction', 'choose', 'choose', 'terrific', 'options', 'consider', 'attractive', 'feature', 'options', 'reflect', 'opportunity', 'cost', 'example', 'cartoon', 'moment', 'probably', 'slowly', 'whenever', 'choose', 'choose', 'may', 'attractive', 'feature', 'less', 'attractive', 'expectations', 'hit', 'replace', 'jeans', 'wear', 'jeans', 'almost', 'jeans', 'flavor', 'buy', 'fit', 'crap', 'incredibly', 'uncomfortable', 'wear', 'wash', 'enough', 'replace', 'jeans', 'wear', 'old', 'ones', 'pair', 'jeans', 'size', 'fit', 'easy', 'fit', 'relax', 'fit', 'button', 'fly', 'fly', 'acid', 'wash', 'distress', 'boot', 'cut', 'blah', 'blah', 'jaw', 'drop', 'recover', 'spend', 'hour', 'damn', 'jeans', 'walk', 'store', 'truth', 'best', 'fit', 'jeans', 'ever', 'choice', 'possible', 'felt', 'worse', 'write', 'whole', 'book', 'explain', 'myself', 'reason', 'reason', 'felt', 'worse', 'options', 'available', 'expectations', 'pair', 'jeans', 'low', 'particular', 'expectations', 'flavor', 'flavor', 'damn', 'perfect', 'wasnt', 'perfect', 'compare', 'expect', 'disappoint', 'comparison', 'expect', 'options', 'increase', 'expectations', 'options', 'produce', 'less', 'satisfaction', 'result', 'result', 'market', 'wait', 'disappoint', 'wouldnt', 'truth', 'everything', 'worse', 'reason', 'everything', 'everything', 'worse', 'everything', 'worse', 'possible', 'experience', 'pleasant', 'surprise', 'industrialize', 'citizens', 'perfection', 'expectation', 'best', 'ever', 'hope', 'stuff', 'expect', 'surprise', 'expectations', 'expectations', 'roof', 'secret', 'happiness', 'secret', 'happiness', 'low', 'expectations', 'moment', 'marry', 'wife', 'shes', 'quite', 'wonderful', 'couldnt', 'settle', 'settle', 'isnt', 'always', 'such', 'bad', 'consequence', 'buy', 'bad', 'fit', 'pair', 'jeans', 'buy', 'whos', 'responsible', 'answer', 'clear', 'responsible', 'hundreds', 'style', 'jeans', 'available', 'buy', 'disappoint', 'whos', 'responsible', 'equally', 'clear', 'answer', 'hundred', 'kinds', 'jeans', 'display', 'excuse', 'failure', 'decisions', 'though', 'result', 'decisions', 'disappoint', 'blame', 'themselves', 'depression', 'explode', 'industrial', 'generation', 'believe', 'significant', 'significant', 'explosion', 'depression', 'suicide', 'experience', 'disappoint', 'standards', 'high', 'explain', 'experience', 'themselves', 'fault', 'net', 'result', 'general', 'worse', 'remind', 'official', 'true', 'false', 'true', 'choice', 'none', 'doesnt', 'follow', 'choice', 'choice', 'magical', 'amount', 'pretty', 'confident', 'since', 'pass', 'options', 'improve', 'welfare', 'policy', 'matter', 'almost', 'policy', 'matter', 'enable', 'choice', 'industrial', 'societies', 'material', 'several', 'too', 'choice', 'too', 'stuff', 'peculiar', 'modern', 'societies', 'frustrate', 'yesterday', 'expensive', 'difficult', 'install', 'child', 'seat', 'waste', 'money', 'expensive', 'complicate', 'choices', 'simply', 'hurt', 'worse', 'enable', 'societies', 'choices', 'shift', 'societies', 'too', 'options', 'improve', 'ours', 'improve', 'economists', 'improve', 'everyone', 'poor', 'excess', 'choice', 'plague', 'conclude', 'anything', 'limit', 'suppose', 'read', 'cartoon', 'sophisticate', 'person', 'fish', 'nothing', 'possible', 'imagination', 'view', 'read', 'however', 'view', 'fish', 'truth', 'matter', 'shatter', 'everything', 'possible', 'freedom', 'paralysis', 'shatter', 'everything', 'possible', 'decrease', 'satisfaction', 'increase', 'paralysis', 'decrease', 'satisfaction', 'almost', 'certainly', 'too', 'limit', 'perhaps', 'fish', 'certainly', 'absence', 'recipe', 'misery', 'suspect', 'disaster'] \n",
      "\n",
      "Plot, in corpus format:\n",
      " [(989, 3), (364, 2), (433, 2), (429, 2), (3308, 1), (859, 2), (520, 1), (1647, 1), (515, 2), (1474, 2), (202, 2), (1898, 5), (2754, 3), (3409, 5), (6, 1), (3923, 5), (3924, 4), (1174, 2), (1520, 1), (2186, 8), (175, 5), (1088, 1), (2098, 1), (1333, 1), (3925, 1), (3057, 1), (2038, 1), (1075, 3), (2259, 1), (986, 20), (2302, 2), (2563, 2), (226, 1), (2707, 1), (1677, 2), (2709, 1), (1067, 1), (945, 1), (412, 2), (2711, 1), (936, 7), (1561, 3), (1566, 2), (163, 1), (3926, 6), (1838, 6), (969, 1), (2991, 1), (1953, 1), (3826, 1), (1559, 1), (612, 11), (1085, 1), (320, 1), (121, 2), (897, 2), (2298, 7), (787, 5), (2434, 1), (617, 2), (3927, 2), (1196, 1), (3928, 3), (3294, 1), (2403, 3), (3929, 1), (693, 1), (135, 2), (641, 1), (79, 1), (1037, 1), (1135, 2), (923, 1), (863, 1), (3930, 1), (3931, 1), (108, 1), (2841, 1), (519, 1), (479, 1), (341, 8), (274, 3), (352, 1), (50, 1), (1663, 6), (3932, 1), (13, 1), (960, 1), (992, 5), (18, 1), (2567, 1), (506, 1), (1945, 1), (1883, 1), (1391, 1), (52, 1), (922, 1), (3262, 1), (2519, 3), (97, 1), (408, 2), (933, 10), (710, 2), (189, 6), (3933, 1), (584, 4), (290, 2), (295, 3), (1048, 1), (738, 1), (401, 6), (1145, 4), (1151, 4), (137, 6), (3934, 2), (3935, 1), (314, 1), (306, 2), (1144, 1), (1997, 1), (622, 7), (28, 2), (3591, 2), (1228, 2), (2636, 4), (407, 1), (2982, 1), (1076, 3), (1659, 1), (670, 3), (2084, 1), (728, 3), (3936, 1), (1402, 1), (2021, 1), (283, 2), (1345, 4), (0, 3), (3541, 2), (2391, 2), (180, 6), (543, 1), (304, 1), (3937, 1), (383, 2), (679, 1), (471, 2), (25, 1), (508, 1), (65, 2), (2104, 2), (1137, 1), (484, 1), (3938, 1), (3939, 1), (976, 2), (1200, 5), (237, 2), (216, 8), (2736, 1), (197, 1), (321, 1), (935, 1), (1451, 1), (3756, 2), (621, 7), (1642, 1), (572, 4), (529, 1), (416, 1), (3336, 1), (650, 2), (1457, 1), (2970, 1), (1405, 1), (287, 1), (602, 3), (88, 2), (196, 1), (674, 1), (1496, 1), (704, 1), (804, 1), (661, 3), (223, 1), (406, 2), (20, 1), (2477, 3), (1322, 2), (1042, 1), (3940, 1), (3941, 1), (348, 1), (3525, 1), (300, 2), (1060, 1), (802, 1), (2782, 1), (530, 1), (232, 4), (3942, 1), (90, 1), (799, 1), (555, 2), (3943, 1), (249, 2), (1926, 1), (618, 4), (553, 2), (552, 4), (925, 1), (16, 2), (289, 5), (569, 1), (220, 2), (3944, 5), (145, 1), (2610, 1), (616, 10), (906, 12), (1552, 2), (964, 3), (1453, 1), (571, 1), (3945, 1), (3946, 1), (537, 1), (533, 1), (1572, 1), (1209, 1), (550, 1), (3947, 1), (2628, 3), (1308, 6), (132, 1), (961, 2), (3948, 3), (1162, 1), (719, 1), (3447, 2), (1427, 2), (51, 1), (3150, 4), (647, 2), (126, 1), (146, 1), (3174, 5), (480, 1), (1092, 1), (1109, 1), (43, 1), (775, 2), (37, 3), (3, 2), (848, 3), (2643, 2), (1267, 1), (3949, 1), (557, 1), (1568, 1), (1938, 2), (2388, 1), (3950, 1), (1723, 1), (110, 2), (15, 1), (1916, 1), (2069, 1), (3951, 2), (1423, 2), (2858, 3), (578, 3), (147, 3), (890, 2), (2256, 1), (3877, 3), (3912, 5), (627, 1), (127, 2), (3952, 6), (3594, 1), (3783, 2), (451, 3), (483, 3), (278, 1), (2187, 1), (1553, 2), (3953, 2), (96, 2), (3954, 4), (882, 3), (3404, 1), (161, 1), (384, 3), (1113, 3), (1366, 2), (1131, 1), (715, 2), (488, 1), (2024, 3), (3955, 1), (546, 1), (781, 1), (1161, 1), (1704, 1), (773, 2), (1872, 1), (3956, 1), (1230, 1), (1889, 2), (365, 2), (257, 1), (2732, 1), (3357, 1), (19, 1), (3957, 7), (475, 1), (682, 2), (998, 11), (795, 3), (3857, 3), (833, 6), (3093, 1), (1183, 1), (1903, 1), (1830, 2), (353, 1), (952, 1), (1460, 3), (872, 1), (3958, 1), (744, 1), (461, 2), (3692, 1), (3959, 1), (463, 1), (695, 1), (3960, 2), (3961, 1), (931, 1), (3962, 1), (1285, 1), (1008, 3), (82, 2), (1949, 2), (994, 7), (4, 1), (1002, 2), (500, 2), (436, 1), (60, 1), (1499, 1), (1365, 2), (892, 1), (3963, 1), (1732, 2), (1489, 1), (3964, 1), (3965, 1), (1291, 1), (1066, 2), (3280, 2), (336, 1), (61, 1), (606, 1), (372, 1), (380, 1), (1751, 2), (182, 1), (398, 1), (720, 2), (418, 3), (1816, 2), (807, 1), (1854, 1), (1543, 1), (3288, 1), (1646, 1), (1478, 1), (1426, 1), (3653, 1), (798, 1), (864, 1), (2723, 2), (2675, 1), (2349, 1), (143, 1), (1658, 1), (586, 1), (225, 1), (3966, 1), (2381, 1), (2323, 1), (1397, 1), (2293, 1), (1626, 1), (2932, 1), (805, 1), (69, 1), (3967, 1), (886, 4), (1083, 2), (1601, 1), (1796, 1), (319, 1), (3968, 1), (152, 1), (1922, 1), (1099, 1), (3210, 1), (990, 1), (1752, 1), (1207, 1), (1177, 1), (2997, 1), (3969, 1), (213, 1), (700, 2), (1735, 2), (3239, 1), (2840, 3), (446, 1), (268, 2), (1431, 1), (3630, 2), (3970, 2), (2250, 1), (3971, 1), (3972, 1), (3915, 1), (3610, 1), (2439, 1)]\n"
     ]
    }
   ],
   "source": [
    "#Creating the Corpus\n",
    "sample = 30\n",
    "corpus = []\n",
    "for plot in plots_tok:\n",
    "    plot_count = Counter(plot)\n",
    "    corpus_doc = []\n",
    "    for item in plot_count.items():\n",
    "        pair = (word2id[item[0]], item[1])\n",
    "        corpus_doc.append(pair)\n",
    "    corpus.append(corpus_doc)\n",
    "\n",
    "print(\"Plot, tokenized:\\n\", plots_tok[sample], \"\\n\")\n",
    "print(\"Plot, in corpus format:\\n\", corpus[sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 5s, sys: 532 ms, total: 1min 6s\n",
      "Wall time: 36 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                            id2word=id2word,\n",
    "                                            num_topics=10, \n",
    "                                            random_state=100,\n",
    "                                            update_every=1,\n",
    "                                            chunksize=100,\n",
    "                                            passes=10,\n",
    "                                            alpha='auto',\n",
    "                                            per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>word rank</th> <th>topic 0</th> <th>topic 1</th> <th>topic 2</th> <th>topic 3</th> <th>topic 4</th> <th>topic 5</th> <th>topic 6</th> <th>topic 7</th> <th>topic 8</th> <th>topic 9</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>1        </td> <td>company  </td> <td>planet  </td> <td>social    </td> <td>play      </td> <td>cancer  </td> <td>guy       </td> <td>water  </td> <td>data       </td> <td>space     </td> <td>women     </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2        </td> <td>money    </td> <td>species </td> <td>political </td> <td>sound     </td> <td>health  </td> <td>write     </td> <td>energy </td> <td>technology </td> <td>image     </td> <td>children  </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>3        </td> <td>countries</td> <td>animals </td> <td>group     </td> <td>game      </td> <td>body    </td> <td>word      </td> <td>plant  </td> <td>machine    </td> <td>art       </td> <td>men       </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>4        </td> <td>dollars  </td> <td>fish    </td> <td>power     </td> <td>music     </td> <td>cells   </td> <td>ever      </td> <td>food   </td> <td>computer   </td> <td>light     </td> <td>family    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>5        </td> <td>country  </td> <td>water   </td> <td>believe   </td> <td>body      </td> <td>disease </td> <td>spend     </td> <td>climate</td> <td>information</td> <td>experience</td> <td>young     </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>6        </td> <td>market   </td> <td>tree    </td> <td>against   </td> <td>video     </td> <td>drug    </td> <td>run       </td> <td>carbon </td> <td>science    </td> <td>paint     </td> <td>man       </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>7        </td> <td>city     </td> <td>ocean   </td> <td>between   </td> <td>hand      </td> <td>patients</td> <td>bite      </td> <td>grow   </td> <td>example    </td> <td>project   </td> <td>parent    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>8        </td> <td>global   </td> <td>star    </td> <td>reason    </td> <td>listen    </td> <td>blood   </td> <td>person    </td> <td>air    </td> <td>model      </td> <td>city      </td> <td>mother    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>9        </td> <td>cities   </td> <td>planets </td> <td>government</td> <td>arm       </td> <td>medical </td> <td>too       </td> <td>solar  </td> <td>image      </td> <td>beautiful </td> <td>woman     </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>10       </td> <td>million  </td> <td>sea     </td> <td>may       </td> <td>head      </td> <td>doctor  </td> <td>read      </td> <td>gas    </td> <td>process    </td> <td>color     </td> <td>home      </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>11       </td> <td>business </td> <td>refugees</td> <td>society   </td> <td>dance     </td> <td>care    </td> <td>job       </td> <td>produce</td> <td>robot      </td> <td>draw      </td> <td>black     </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>12       </td> <td>job      </td> <td>forest  </td> <td>case      </td> <td>voice     </td> <td>baby    </td> <td>maybe     </td> <td>fuel   </td> <td>robots     </td> <td>film      </td> <td>old       </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>13       </td> <td>pay      </td> <td>humans  </td> <td>war       </td> <td>experience</td> <td>study   </td> <td>everything</td> <td>eat    </td> <td>pattern    </td> <td>house     </td> <td>experience</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>14       </td> <td>cost     </td> <td>ice     </td> <td>state     </td> <td>record    </td> <td>medicine</td> <td>car       </td> <td>fly    </td> <td>number     </td> <td>wall      </td> <td>speak     </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>15       </td> <td>economic </td> <td>land    </td> <td>example   </td> <td>eye       </td> <td>die     </td> <td>name      </td> <td>cloud  </td> <td>object     </td> <td>sense     </td> <td>fear      </td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_topics = 10\n",
    "num_words = 15\n",
    "top_words = Table().with_column('word rank', np.arange(1,num_words+1))\n",
    "for k in np.arange(num_topics): \n",
    "    topic = lda_model.get_topic_terms(k, num_words)\n",
    "    words = [id2word[topic[i][0]] for i in np.arange(num_words)]\n",
    "    probs = [topic[i][1] for i in np.arange(num_words)]\n",
    "    top_words = top_words.with_column('topic %d' % k, words)\n",
    "    \n",
    "top_words.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>Topic</th> <th>Probabilities</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>0    </td> <td>0.00198656   </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>1    </td> <td>0.00127246   </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2    </td> <td>0.0105815    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>3    </td> <td>0.205505     </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>4    </td> <td>0.000858629  </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>5    </td> <td>0.0503681    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>6    </td> <td>0.0108479    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>7    </td> <td>0.646343     </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>8    </td> <td>0.0696848    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>9    </td> <td>0.00255206   </td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic with highest probability: 7 (0.646343)\n"
     ]
    }
   ],
   "source": [
    "sample = 13\n",
    "topic_dist = lda_model.get_document_topics(corpus[sample], minimum_probability = 0)\n",
    "topics = [pair[0] for pair in topic_dist] \n",
    "probabilities = [pair[1] for pair in topic_dist]\n",
    "topic_dist_table = Table().with_columns('Topic', topics, 'Probabilities', probabilities)\n",
    "topic_dist_table.show(20)\n",
    "t = np.argmax(probabilities)\n",
    "print(\"Topic with highest probability: %d (%f)\" % (t, probabilities[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " really excited to be here today show you some stuff thats just ready to come out of the lab literally and really glad that you guys are going to be among the first to see it in person because really think this is going to really change the way we interact with machines from this point on this is a rear projected drafting table about 36 inches wide and its equipped with a multi touch sensor touch sensors that you see like on a kiosk or interactive whiteboards can only register one point of contact at a time thing allows you to have multiple points at the same time can use both my hands can use chording actions can just go right up and use all 10 fingers if wanted to know like that multi touch sensing isnt completely new like have been playing around with it in the 80s the approach built here is actually high resolution low cost and probably most importantly very scalable the technology you know isnt the most exciting thing here right now other than probably its newfound accessibility really interesting here is what you can do with it and the kind of interfaces you can build on top of it lets see for instance we have a lava lamp application here you can see can use both of my hands to kind of squeeze and put the blobs together can inject heat into the system here or can pull it apart with two of my fingers completely intuitive theres no instruction manual interface just kind of disappears started out as a screensaver app that one of the students in our lab made think its true identity comes out here whats great about a multi touch sensor is that you know could be doing this with as many fingers here but of course multi touch also inherently means multi user could be interacting with another part of while play around with it here can imagine a new kind of sculpting tool where kind of warming something up making it malleable and then letting it cool down and solidifying in a certain state should have something like this in their lobby show you a little more of a concrete example here as this thing loads is a photographers light box application can use both of my hands to interact and move photos around whats even cooler is that if have two fingers can actually grab a photo and then stretch it out like that really easily can pan zoom and rotate it effortlessly can do that grossly with both of my hands or can do it just with two fingers on each of my hands together grab the canvas can do the same thing stretch it out can do it simultaneously holding this down a\n"
     ]
    }
   ],
   "source": [
    "print(transcripts[sample][0:2500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, Topic 7, which represents technology, has the highest probability with .646. Looking at the transcript of the talk, we see that this is in fact true. In the transcript of this sample, we see terms like \"screensaver\", \"touch sensor\", and \"interactive\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>Topic</th> <th>Probabilities</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>0    </td> <td>0.172373     </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>1    </td> <td>0.0142661    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2    </td> <td>0.0366336    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>3    </td> <td>0.000343127  </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>4    </td> <td>0.000314115  </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>5    </td> <td>0.0978113    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>6    </td> <td>0.000276459  </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>7    </td> <td>0.159664     </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>8    </td> <td>0.514234     </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>9    </td> <td>0.00408484   </td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic with highest probability: 8 (0.514234)\n"
     ]
    }
   ],
   "source": [
    "sample = 7\n",
    "topic_dist = lda_model.get_document_topics(corpus[sample], minimum_probability = 0)\n",
    "probabilities = [pair[1] for pair in topic_dist]\n",
    "topics = [pair[0] for pair in topic_dist]\n",
    "topic_dist_table = Table().with_columns('Topic', topics, 'Probabilities', probabilities)\n",
    "topic_dist_table.show(20)\n",
    "t = np.argmax(probabilities)\n",
    "print(\"Topic with highest probability: %d (%f)\" % (t, probabilities[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " going to present three projects in rapid fire dont have much time to do it want to reinforce three ideas with that rapid fire presentation first is what like to call a hyper rational process a process that takes rationality almost to an absurd level and it transcends all the baggage that normally comes with what people would call sort of a rational conclusion to something it concludes in something that you see here that you actually wouldnt expect as being the result of rationality second the second is that this process does not have a signature is no authorship are obsessed with authorship is something that has editing and it has teams but in fact we no longer see within this process the traditional master architect creating a sketch that his minions carry out the third is that it challenges and this is in the length of this very hard to support why connect all these things but it challenges the high modernist notion of flexibility modernists said we will create sort of singular spaces that are generic almost anything can happen within them call it sort of shotgun flexibility turn your head this way shoot and youre bound to kill something this is the promise of high modernism within a single space actually any kind of activity can happen as were seeing operational costs are starting to dwarf capital costs in terms of design parameters so with this sort of idea what happens is whatever actually is in the building on opening day or whatever seems to be the most immediate need starts to dwarf the possibility and sort of subsume it of anything else could ever happen so were proposing a different kind of flexibility something that we call compartmentalized flexibility the idea is that you within that continuum identify a series of points and you design specifically to them can be pushed off center a little bit but in the end you actually still get as much of that original spectrum as you originally had hoped high modernist flexibility that doesnt really work going to talk about going to build up the in this way before your eyes in about five or six diagrams and truly mean this is the design process that youll see the library staff and the library board we settled on two core positions is the first one and this is showing over the last 900 years the evolution of the book and other technologies diagram was our sort of position piece about the book and our position was books are technology thats something people forget but its a form of technology that will have\n"
     ]
    }
   ],
   "source": [
    "print(transcripts[sample][0:2500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this sample, we observe that topic 8, which represents art, has the highest probability with .51. Looking at the transcript, we can see that our topic model is correct since there are terms like \"modernists\", \"design\", and \"diagram\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>Topic</th> <th>Probabilities</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>0    </td> <td>0.00560299   </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>1    </td> <td>0.0331117    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2    </td> <td>0.00944252   </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>3    </td> <td>0.00036272   </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>4    </td> <td>0.641746     </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>5    </td> <td>0.139144     </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>6    </td> <td>0.0233119    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>7    </td> <td>0.098927     </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>8    </td> <td>0.00981698   </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>9    </td> <td>0.0385338    </td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic with highest probability: 4 (0.641746)\n"
     ]
    }
   ],
   "source": [
    "sample = 31\n",
    "topic_dist = lda_model.get_document_topics(corpus[sample], minimum_probability = 0)\n",
    "probabilities = [pair[1] for pair in topic_dist]\n",
    "topics = [pair[0] for pair in topic_dist]\n",
    "topic_dist_table = Table().with_columns('Topic', topics, 'Probabilities', probabilities)\n",
    "topic_dist_table.show(20)\n",
    "t = np.argmax(probabilities)\n",
    "print(\"Topic with highest probability: %d (%f)\" % (t, probabilities[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " you really an honor and a privilege to be here spending my last day as a teenager want to talk to you about the future but first going to tell you a bit about the past story starts way before was born grandmother was on a train to the death camp she was going along the tracks and the tracks split somehow we dont really know exactly the whole story but the train took the wrong track and went to a work camp rather than the death camp grandmother survived and married my grandfather were living in and my mother was born when my mother was two years old the revolution was raging and they decided to escape got on a boat and yet another divergence the boat was either going to or to got on and didnt know where they were going and ended up in to make a long story short they came to grandmother was a chemist worked at the in and at 44 she died of stomach cancer never met my grandmother but carry on her name her exact name and like to think carry on her scientific passion too found this passion not far from here actually when was nine years old family was on a road trip and we were in the had never been a reader when was young my dad had tried me with the tried tried all that and just didnt like reading books my mother bought this book when we were at the called was all about the outbreak of the virus something about it just kind of drew me towards it was this big sort of bumpy looking virus on the cover and just wanted to read it picked up that book and as we drove from the edge of the to and to actually here where we are today in read that book and from when was reading that book knew that wanted to have a life in medicine wanted to be like the explorers read about in the book who went into the jungles of went into the research labs and just tried to figure out what this deadly virus was from that moment on read every medical book could get my hands on and just loved it so much was a passive observer of the medical world wasnt until entered high school that thought now you know being a big high school kid can maybe become an active part of this big medical world was 14 and emailed professors at the local university to see if maybe could go work in their lab hardly anyone responded mean why would they respond to a 14 year old anyway got to go talk to one professor who accepted me into the lab that time was really interested in neuroscience and wanted to do a research project in neurology specifically looking at the effects of heavy metals on the developing nervous \n"
     ]
    }
   ],
   "source": [
    "print(transcripts[sample][0:2500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The topic with the highest probability of .64 is topic 4, which represents medicine. Looking at the transcript of the talk, we observe that our topic model was able to correctly identify the topic at hand. Terms like \"cancer\", \"medical\", and \"research\" were all used in this TED talk."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
